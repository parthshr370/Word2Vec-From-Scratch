{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec model word2vec.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parthshr370/parthpython/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2vec(L.LightningModule):\n",
    "    \n",
    "    def __init__(self,vocab_size , embedding_dim , learning_rate = 0.01):\n",
    "        \n",
    "        super(Word2vec,self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim,vocab_size)\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        output = self.linear(embeds)\n",
    "        \n",
    "        log_probablity = F.log_softmax(output,dim=1)\n",
    "        return log_probablity    \n",
    "    \n",
    "    def training_steps(self,batch,batch_idx):\n",
    "        inputs , targets = batch\n",
    "        \n",
    "        logs_prob = self(inputs)\n",
    "        loss = F.nll_loss(logs_prob,targets)\n",
    "        self.log('training loss ' , loss)\n",
    "        return loss \n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimiser = torch.optim.adam(self.parameters(),lr = self.learning_rate)\n",
    "        return optimiser\n",
    "    \n",
    "    def get_word_embedding(self):\n",
    "        return self.embeddings.weight.data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with Dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import numpy as np \n",
    "import re # this is for regular expression \n",
    "from typing import List, Tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Corpus of text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self,file_path:str):\n",
    "        self.file_path = file_path\n",
    "        self.words = self.read_corpus()\n",
    "        self.word_counts = Counter(self.words)\n",
    "        self.vocab = self.build_vocab()\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(self.vocab)} # dictionary mapping from word to inext \n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()} # dictionary mapping index to word for retreival when training testing \n",
    "        \n",
    "        \n",
    "    def read_corpus(self) -> List[str]:\n",
    "        with open(self.file_path,'r',encoding= 'utf-7') as f:\n",
    "            return [word.lower() for line in f for word in re.findall(r'\\w+', line) ]\n",
    "        \n",
    "    def build_vocab(self,min_count: int = 5) -> List[str]:\n",
    "        return [word for word, count in self.word_counts.items() if count >= min_count]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2vecDataset(Dataset):\n",
    "    def __init__(self,corpus: Corpus , window_size: int=5):\n",
    "        self.corpus  = corpus\n",
    "        self.window_size = window_size\n",
    "        self.data = self.create_dataset() # we will define this function later on \n",
    "        \n",
    "    def create_dataset(self) -> List[tuple[int,int]]:\n",
    "        data = [] # define the vector that contains the dataset \n",
    "        for i , target in enumerate(self.corpus.words):\n",
    "            target_idx = self.corpus.word2idx.get(target)\n",
    "            if target_idx is None:\n",
    "                continue\n",
    "            context_words = self.corpus.words[max(0, i - self.window_size):i] + \\\n",
    "                            self.corpus.words[i + 1:i + 1 + self.window_size]\n",
    "            for context in context_words:\n",
    "                context_idx = self.corpus.word2idx.get(context)\n",
    "                if context_idx is not None:\n",
    "                    data.append((target_idx, context_idx))\n",
    "        return data\n",
    "    \n",
    "    def __len__(self): # Returns the total number of samples\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx): # returns the returns a specific sample in pytorch\n",
    "        return torch.tensor(self.data[idx][0]), torch.tensor(self.data[idx][1]) # returns two rensors with index 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Word2vecDataModule(L.LightningDataModule):\n",
    "    def __init__(self, file_path: str, batch_size : int = 64):\n",
    "        super().__init__()\n",
    "        self.file_path = file_path\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def setup(self, stage= None): # initializes the necessary components for training a Word2Vec model using text data from a specified file path\n",
    "        corpus = Corpus(self.file_path)\n",
    "        self.datataset = Word2vecDataset(corpus)\n",
    "        self.vocab_size = len(corpus.vocab)\n",
    "        \n",
    "        \n",
    "    def train_dataloader(self) :\n",
    "        return DataLoader(self.dataset, batch_size=self.batch_size,shuffle=True,num_workers=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main model skipgram and others "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip gram model nn.Module :\n",
    "\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size: int, embedding_dim : int):\n",
    "        super(SkipGram,self).__init__()\n",
    "        self.embeding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.output = nn.Linear(embedding_dim,vocab_size)\n",
    "        \n",
    "    def forward (self,inputs):\n",
    "        embeds = self.embeding(inputs)\n",
    "        output = self.output(embeds)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecLightning(L.LightningModule):\n",
    "    \n",
    "    def __init__():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fastapi'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastAPI\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_word2vec\n\u001b[1;32m      5\u001b[0m app \u001b[38;5;241m=\u001b[39m FastAPI()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fastapi'"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Word2VecLightningModule' from 'src.model.word2vec' (/home/parthshr370/Downloads/Pytorch Practice/word2vec/src/model/word2vec.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastAPI\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_word2vec\n\u001b[1;32m      6\u001b[0m app \u001b[38;5;241m=\u001b[39m FastAPI()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;129m@app\u001b[39m\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/train\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(file_path: \u001b[38;5;28mstr\u001b[39m, embedding_dim:\u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m , batch_size : \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m,max_epochs : \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m):\n",
      "File \u001b[0;32m~/Downloads/Pytorch Practice/word2vec/src/training/train.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2vecDataModule\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mword2vec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2VecLightningModule\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_word2vec\u001b[39m(file_path: \u001b[38;5;28mstr\u001b[39m, embedding_dim: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m,batch_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m, max_epochs: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m     10\u001b[0m     data_module \u001b[38;5;241m=\u001b[39m Word2vecDataModule(file_path,batch_size)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Word2VecLightningModule' from 'src.model.word2vec' (/home/parthshr370/Downloads/Pytorch Practice/word2vec/src/model/word2vec.py)"
     ]
    }
   ],
   "source": [
    "import fastapi\n",
    "from fastapi import FastAPI\n",
    "\n",
    "from src.training.train import train_word2vec\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/train\")\n",
    "async def train_model(file_path: str, embedding_dim:int = 300 , batch_size : int = 64,max_epochs : int = 5):\n",
    "    model,data_module = train_word2vec(file_path,embedding_dim,batch_size,max_epochs)\n",
    "    return  {\"message\": \"model Trained Succesfully\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parthpython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
